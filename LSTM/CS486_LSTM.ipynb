{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS486 LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Da-Pen/CS486-twitter-bot/blob/main/LSTM/CS486_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ULr12ZmoiSd"
      },
      "source": [
        "Preprocessing Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brrqg7CpoMLw"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# CONSTANTS\n",
        "NEWS_ORGS_DATA_FILE_NAME = '/content/data/newsorgs_data'\n",
        "TRUMP_DATA_FILE_NAME = '/content/data/donald_trump_data'\n",
        "SKIP_URLS = True\n",
        "SKIP_ELLIPSES = True\n",
        "SKIP_RETWEETS = True\n",
        "SKIP_REPLIES = True     # it seems like Trump often has tweets where he simply replies to another Twitter user or quotes them. They usually start with '@' or '\"@'. If this is set to true, then ignore those tweets.\n",
        "ONLY_LOWERCASE = True  # if set to True, convert all text to lowercase\n",
        "VALID_CHARS = [\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '\\n',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
        "    ':', ';', '?', '@', \n",
        "    '_', \n",
        "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \n",
        "    '—', '’', '“', '”'\n",
        "]\n",
        "if not ONLY_LOWERCASE:\n",
        "    VALID_CHARS = VALID_CHARS + ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "VALID_CHARS_SET = set(VALID_CHARS)\n",
        "\n",
        "CHAR_TO_INDEX = dict((c, i) for i, c in enumerate(VALID_CHARS))\n",
        "INDEX_TO_CHAR = dict((i, c) for i, c in enumerate(VALID_CHARS))\n",
        "\n",
        "MIN_VALID_CHAR_PERCENT = 0.9  # at least this ratio of the characters in the tweet have to be valid (i.e have to exist in the above grammar). Otherwise we ignore the tweet.\n",
        "\n",
        "\n",
        "# returns a string minus all the urls in it\n",
        "def ignore_urls(s):\n",
        "    return ' '.join([x for x in s.split() if 'http' not in x])\n",
        "\n",
        "\n",
        "# filters invalid characters. If the number of characters filtered is greater than (1 - MIN_VALID_CHAR_PERCENT), then return None.\n",
        "def filter_invalid_chars(tweet):\n",
        "    new_tweet = ''.join([char for char in tweet if char in VALID_CHARS_SET])\n",
        "    if len(new_tweet) / len(tweet) < MIN_VALID_CHAR_PERCENT:\n",
        "        # print('ignoring tweet (too few valid characters):', tweet)\n",
        "        return None\n",
        "    # elif len(new_tweet) < len(tweet):\n",
        "    #     print('filtered tweet. Old:', tweet, 'New:', new_tweet)\n",
        "    return new_tweet\n",
        "\n",
        "\n",
        "# gets a list of strings representing the tweets in the given file.\n",
        "# can limit the number of tweets to get using upto.\n",
        "# replaces 'NEWLINE's with actual \\n characters.\n",
        "def get_tweets_list(filename, upto=None):\n",
        "    f = open(filename, 'r')\n",
        "    lines = f.read().split('\\n')[:upto]\n",
        "    f.close()\n",
        "    # replace NEWLINE's and ignore all lines that do not have spaces (because they are probably just a link)\n",
        "    lines = [line.replace('NEWLINE', '\\n') for line in lines if line.strip().find(' ') != -1]\n",
        "    if ONLY_LOWERCASE:\n",
        "        lines = [line.lower() for line in lines]\n",
        "    if SKIP_ELLIPSES:  # skip tweets with the '…' character, which indicates that it has been truncated\n",
        "        lines = [line for line in lines if line.find('…') == -1]\n",
        "    if SKIP_URLS:\n",
        "        lines = [ignore_urls(line) for line in lines]\n",
        "    if SKIP_RETWEETS:\n",
        "        lines = [line for line in lines if line[:2] != 'RT']\n",
        "    if SKIP_REPLIES:\n",
        "        lines = [line for line in lines if line[0] != '@' and line[:2] != '\"@']\n",
        "    # check what percentage of characters are valid: if less than MIN_VALID_CHAR_PERCENT are valid, then ignore this tweet. Otherwise, delete invalid characters.\n",
        "    lines = [filter_invalid_chars(line) for line in lines if filter_invalid_chars(line) is not None]\n",
        "    return np.array(lines)\n",
        "\n",
        "\n",
        "# gets most commonly occuring characters in datasets\n",
        "def get_commonly_occuring_characters():\n",
        "    threshold = 500  # If characters appear more than threshold times in all tweets in all datasets, it is printed\n",
        "    news_org_tweets = get_tweets_list(NEWS_ORGS_DATA_FILE_NAME)\n",
        "    trump_tweets = get_tweets_list(TRUMP_DATA_FILE_NAME)\n",
        "    # see which characters exist\n",
        "    chars_to_occurrence_map = defaultdict(lambda: 0)\n",
        "    for tweet in news_org_tweets:\n",
        "        for char in tweet:\n",
        "            chars_to_occurrence_map[char] += 1\n",
        "    for tweet in trump_tweets:\n",
        "        for char in tweet:\n",
        "            chars_to_occurrence_map[char] += 1\n",
        "    chars_set = set()\n",
        "    for char in chars_to_occurrence_map.keys():\n",
        "        if chars_to_occurrence_map[char] > threshold:\n",
        "            chars_set.add((char, chars_to_occurrence_map[char]))  # if we want to print (char, occurence_times) tuples\n",
        "            # chars_set.add(char)                                     # if we want to print just the characters  \n",
        "    return sorted(list(chars_set))\n",
        "\n",
        "\n",
        "def main():\n",
        "    pass    # do nothing (may comment out if we want to test something)\n",
        "    # news_org_tweets = get_tweets_list(NEWS_ORGS_DATA_FILE_NAME)\n",
        "    # trump_tweets = get_tweets_list(TRUMP_DATA_FILE_NAME)\n",
        "    # print('\\n'.join(trump_tweets))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF3x9jXSpe5B"
      },
      "source": [
        "Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK_YqVArpeDg",
        "outputId": "9f8609c6-7c9c-44ad-c430-cec0ea183347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Bidirectional, BatchNormalization, Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "INPUT_LENGTH = 40  # based on INPUT_LENGTH characters, our model generates the next character\n",
        "GENERATED_TWEET_LENGTH = 120\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _, data, model):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    for _ in range(2):     # use 10 different tweets as samples\n",
        "        tweet = np.random.choice(data) # select random tweet\n",
        "        start_index = 0\n",
        "\n",
        "        for diversity in [0.2, 0.4, 0.6, 1.0]:\n",
        "        # for diversity in [0.1, 0.2, 0.3, 0.4]:\n",
        "        # for diversity in [0.3, 0.4, 0.5]:\n",
        "            print('----- diversity:', diversity)\n",
        "\n",
        "            generated = ''\n",
        "            sentence = tweet[start_index: start_index + INPUT_LENGTH]\n",
        "            generated += sentence\n",
        "            print('----- Generating with seed: \"' + sentence + '\"')\n",
        "            sys.stdout.write(generated)\n",
        "\n",
        "            for i in range(GENERATED_TWEET_LENGTH):\n",
        "                x_pred = np.zeros((1, INPUT_LENGTH, len(VALID_CHARS)))\n",
        "                for t, char in enumerate(sentence):\n",
        "                    x_pred[0, t, CHAR_TO_INDEX[char]] = 1.\n",
        "\n",
        "                preds = model.predict(x_pred, verbose=0)[0]\n",
        "                next_index = sample(preds, diversity)\n",
        "                next_char = INDEX_TO_CHAR[next_index]\n",
        "\n",
        "                generated += next_char\n",
        "                sentence = sentence[1:] + next_char\n",
        "\n",
        "                sys.stdout.write(next_char)\n",
        "                sys.stdout.flush()\n",
        "            print()\n",
        "\n",
        "\n",
        "def train_from_data(data):\n",
        "    # convert the raw tweets list to input and output\n",
        "    # input is equal to INPUT_LENGTH characters, output is a single character\n",
        "    sentences = []\n",
        "    next_chars = []\n",
        "    for x in data:\n",
        "        for i in range(0, len(x) - INPUT_LENGTH):\n",
        "            sentences.append(x[i: i + INPUT_LENGTH])\n",
        "            next_chars.append(x[i + INPUT_LENGTH])\n",
        "    print('# training samples:', len(sentences))\n",
        "    # for i in range(10):\n",
        "    #     print(sentences[i],'->',next_chars[i])\n",
        "\n",
        "    # vectorize the data\n",
        "    print('Vectorization...')\n",
        "    x = np.zeros((len(sentences), INPUT_LENGTH, len(VALID_CHARS)), dtype=np.bool)\n",
        "    y = np.zeros((len(sentences), len(VALID_CHARS)), dtype=np.bool)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[i, t, CHAR_TO_INDEX[char]] = 1\n",
        "        y[i, CHAR_TO_INDEX[next_chars[i]]] = 1\n",
        "\n",
        "    # build the model\n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(len(VALID_CHARS) * 7, input_shape=(INPUT_LENGTH, len(VALID_CHARS))))\n",
        "    \n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('selu'))\n",
        "\n",
        "    model.add(Dense(len(VALID_CHARS)*4))\n",
        "    model.add(Activation('selu'))\n",
        "\n",
        "    model.add(Dense(len(VALID_CHARS)*4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('selu'))\n",
        "\n",
        "    # model.add(Bidirectional(LSTM(128), input_shape=(INPUT_LENGTH, len(VALID_CHARS))))\n",
        "    model.add(Dense(len(VALID_CHARS), activation='softmax'))\n",
        "\n",
        "    # optimizer = RMSprop(lr=0.01)\n",
        "    optimizer = Adam()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_crossentropy', 'accuracy'])\n",
        "\n",
        "    epochs = 10\n",
        "    \n",
        "    print_callback = LambdaCallback(on_epoch_end=lambda a, b: on_epoch_end(a, b, data, model))\n",
        "\n",
        "    # train the model\n",
        "    model.fit(x, y,\n",
        "            epochs=epochs,\n",
        "            callbacks=[print_callback]\n",
        "            )\n",
        "\n",
        "    # save and download the model\n",
        "    model.save('/content/model')\n",
        "    !zip -r /content/model.zip /content/model\n",
        "    files.download('/content/model.zip')\n",
        "\n",
        "def main():\n",
        "    # TRAIN TRUMP\n",
        "    trump_data = get_tweets_list(TRUMP_DATA_FILE_NAME, 1000)  # TODO remove upto\n",
        "    print(\"number of trump tweets:\", len(trump_data))\n",
        "    train_from_data(trump_data)\n",
        "    # TRAIN NEWS ORGS\n",
        "    # TODO\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of trump tweets: 850\n",
            "# training samples: 48813\n",
            "Vectorization...\n",
            "Build model...\n",
            "Epoch 1/10\n",
            "1526/1526 [==============================] - ETA: 0s - loss: 2.5216 - categorical_crossentropy: 2.5216 - accuracy: 0.3111\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"take a look at and to see these beautifu\"\n",
            "take a look at and to see these beautifus frlllve ne tr mp nn tr mp nn tr mp nf n nf n ma n n walll tr mp tr mp nf n nf n tr mp nn tr mp nn nf lll fr mp ramp tr\n",
            "----- diversity: 0.4\n",
            "----- Generating with seed: \"take a look at and to see these beautifu\"\n",
            "take a look at and to see these beautifurlloll be cell nf nr pprrmp nn nf llll bloll hamp of nr mall to llll fr mp rlak ffll tr mall ha nelvel to t ffrmm nat n \n",
            "----- diversity: 0.6\n",
            "----- Generating with seed: \"take a look at and to see these beautifu\"\n",
            "take a look at and to see these beautifur malllu grrl #t mpmrttrget nellolf rall llf t mrl hall be tall held nlve nn nelvert fr mas ralldebt tamp cllbright fex \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"take a look at and to see these beautifu\"\n",
            "take a look at and to see these beautifux2 cl's rex@fbomelj la mryex amps8mprrcon new has mpallling amp clomet  flpm ff ra naldald lra vax a trpmrstrfm wn half \n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"melania and i saw american idiot on broa\"\n",
            "melania and i saw american idiot on broall a lall tr mp mall n n tll n n n n nelll ha llllve dall mall nex namp ramp ramp nn tf n nall frrmp nf fr mall nall fr \n",
            "----- diversity: 0.4\n",
            "----- Generating with seed: \"melania and i saw american idiot on broa\"\n",
            "melania and i saw american idiot on broalllo f mp llll ha wall ne tr ppprrop rramp frr mall bet tax nax allllbe dex nef mas nat llle mell halld ne spramp nf nf \n",
            "----- diversity: 0.6\n",
            "----- Generating with seed: \"melania and i saw american idiot on broa\"\n",
            "melania and i saw american idiot on broall tr prlt mpoll mamp ramp nnig tl nall in tr mall a ncllal ne lff tf mllor ra mat llbl ax collebl hand lf nt grpmm nf t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"melania and i saw american idiot on broa\"\n",
            "melania and i saw american idiot on broalk rlast's lcp rm smes ram sakmelfrr mn offs selamhs sppllin'smrry ha olf ivll f  sppro mps ard olld nb las nowlll dexth\n",
            "1526/1526 [==============================] - 439s 288ms/step - loss: 2.5216 - categorical_crossentropy: 2.5216 - accuracy: 0.3111\n",
            "Epoch 2/10\n",
            "1378/1526 [==========================>...] - ETA: 37s - loss: 2.1511 - categorical_crossentropy: 2.1511 - accuracy: 0.3827"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYW326dpsn99"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}