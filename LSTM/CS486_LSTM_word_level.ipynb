{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS486 LSTM word-level.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOszN9PRRwalRSer6pp0P+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Da-Pen/CS486-twitter-bot/blob/main/LSTM/CS486_LSTM_word_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzpWcqefq1_m"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# CONSTANTS\n",
        "NEWS_ORGS_DATA_FILE_NAME = '/content/data/newsorgs_data'\n",
        "TRUMP_DATA_FILE_NAME = '/content/data/donald_trump_data'\n",
        "SKIP_URLS = True\n",
        "SKIP_ELLIPSES = True\n",
        "SKIP_RETWEETS = True\n",
        "SKIP_REPLIES = True     # it seems like Trump often has tweets where he simply replies to another Twitter user or quotes them. They usually start with '@' or '\"@'. If this is set to true, then ignore those tweets.\n",
        "MIN_TWEET_LENGTH = 50 # characters\n",
        "\n",
        "# returns a string minus all the urls in it\n",
        "def ignore_urls(s):\n",
        "    return ' '.join([x for x in s.split() if 'http' not in x])\n",
        "\n",
        "\n",
        "# gets a list of strings representing the tweets in the given file.\n",
        "# can limit the number of tweets to get using upto.\n",
        "# replaces 'NEWLINE's with actual \\n characters.\n",
        "def get_tweets_list(filename, upto=None):\n",
        "    f = open(filename, 'r')\n",
        "    lines = f.read().split('\\n')[:upto]\n",
        "    f.close()\n",
        "    # replace NEWLINE's and ignore all lines that do not have spaces (because they are probably just a link)\n",
        "    # lines = [line.replace('NEWLINE', '\\n') for line in lines if line.strip().find(' ') != -1]\n",
        "    # if ONLY_LOWERCASE:\n",
        "    #     lines = [line.lower() for line in lines]\n",
        "    if SKIP_ELLIPSES:  # skip tweets with the '…' character, which indicates that it has been truncated\n",
        "        lines = [line for line in lines if line.find('…') == -1]\n",
        "    if SKIP_URLS:\n",
        "        lines = [ignore_urls(line) for line in lines]\n",
        "    if SKIP_RETWEETS:\n",
        "        lines = [line for line in lines if line[:2] != 'RT']\n",
        "    if SKIP_REPLIES:\n",
        "        lines = [line for line in lines if len(line) > 0 and line[0] != '@' and line[:2] != '\"@']\n",
        "    # # check what percentage of characters are valid: if less than MIN_VALID_CHAR_PERCENT are valid, then ignore this tweet. Otherwise, delete invalid characters.\n",
        "    # lines = [filter_invalid_chars(line) for line in lines if filter_invalid_chars(line) is not None]\n",
        "    return np.array(lines)\n",
        "\n",
        "# given a list of tweets, gets a map of words to occurrences\n",
        "def get_words(tweets):\n",
        "    all_words = defaultdict(lambda: 0)\n",
        "    for tweet in tweets:\n",
        "        words = tweet.split(' ')\n",
        "        for word in words:\n",
        "            all_words[word] += 1\n",
        "    return all_words\n",
        "\n",
        "def get_words_list(words_map):\n",
        "    min_occurrence = 5\n",
        "    words_list = []\n",
        "    for word in words_map.keys():\n",
        "        if words_map[word] > min_occurrence:\n",
        "            words_list.append(word)\n",
        "    return words_list\n",
        "\n",
        "\n",
        "def filter_words(tweet, words_set):\n",
        "    return ' '.join([word for word in tweet.split(' ') if word in words_set])\n",
        "\n",
        "trump_tweets = get_tweets_list(TRUMP_DATA_FILE_NAME)\n",
        "\n",
        "words_list = get_words_list(get_words(trump_tweets))\n",
        "word_to_index = dict((c, i) for i, c in enumerate(words_list))\n",
        "index_to_word = dict((i, c) for i, c in enumerate(words_list))\n",
        "print(len(words_list))\n",
        "print(words_list)\n",
        "words_set = set(words_list)\n",
        "# ignore all invalid words in tweets\n",
        "print(\"BEFORE\", len(trump_tweets))\n",
        "new_trump_tweets = []\n",
        "for tweet in trump_tweets:\n",
        "    filtered_tweet = filter_words(tweet, words_set)\n",
        "    if len(filtered_tweet) > 0.8*len(tweet):\n",
        "        new_trump_tweets.append(filtered_tweet)\n",
        "trump_tweets = new_trump_tweets\n",
        "\n",
        "# filter short tweets\n",
        "trump_tweets = [tweet for tweet in trump_tweets if len(tweet) > MIN_TWEET_LENGTH]\n",
        "\n",
        "print(\"AFTER\", len(trump_tweets))\n",
        "\n",
        "\n",
        "def main():\n",
        "    pass    # do nothing (may comment out if we want to test something)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls0kt3-Nq-0X"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujL6TdR0q-DV"
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Bidirectional, BatchNormalization, Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "import random\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "INPUT_LENGTH = 5  # based on INPUT_LENGTH characters, our model generates the next character\n",
        "GENERATED_TWEET_LENGTH = 20 # words\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _, data, model):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    for _ in range(2):     # use 10 different tweets as samples\n",
        "        tweet = np.random.choice(data) # select random tweet\n",
        "        start_index = 0\n",
        "\n",
        "        for diversity in [0.2, 0.4, 0.6, 1.0]:\n",
        "        # for diversity in [0.1, 0.2, 0.3, 0.4]:\n",
        "        # for diversity in [0.3, 0.4, 0.5]:\n",
        "            print('----- diversity:', diversity)\n",
        "\n",
        "            generated = ''\n",
        "            sentence = tweet.split(' ')[start_index: start_index + INPUT_LENGTH]\n",
        "            generated += ' '.join(sentence)\n",
        "            print('----- Generating with seed: \"' + ' '.join(sentence) + '\"')\n",
        "            # sys.stdout.write(generated)\n",
        "\n",
        "            for i in range(GENERATED_TWEET_LENGTH):\n",
        "                x_pred = np.zeros((1, INPUT_LENGTH, len(words_list)))\n",
        "                for t, word in enumerate(sentence):\n",
        "                    x_pred[0, t, word_to_index[word]] = 1.\n",
        "\n",
        "                preds = model.predict(x_pred, verbose=0)[0]\n",
        "                next_index = sample(preds, diversity)\n",
        "                next_word = index_to_word[next_index]\n",
        "                generated += ' ' + next_word\n",
        "                sentence = sentence[1:] + [next_word]\n",
        "\n",
        "                # sys.stdout.write(next_word)\n",
        "                # sys.stdout.flush()\n",
        "            print(generated)\n",
        "            print()\n",
        "\n",
        "\n",
        "def train_from_data(data, train_limit=None):\n",
        "    # convert the raw tweets list to input and output\n",
        "    # input is equal to INPUT_LENGTH characters, output is a single character\n",
        "    if train_limit:\n",
        "        data = data[:train_limit]\n",
        "    sentences = []\n",
        "    next_words = []\n",
        "    for tweet in data:\n",
        "        tweet_words = tweet.split(' ')\n",
        "        for i in range(0, len(tweet_words) - INPUT_LENGTH):\n",
        "            sentences.append(tweet_words[i: i + INPUT_LENGTH])\n",
        "            next_words.append(tweet_words[i + INPUT_LENGTH])\n",
        "    print('# training samples:', len(sentences))\n",
        "    # for i in range(10):\n",
        "    #     print(sentences[i],'->',next_words[i])\n",
        "\n",
        "    # vectorize the data\n",
        "    print('Vectorization...')\n",
        "    x = np.zeros((len(sentences), INPUT_LENGTH, len(words_list)), dtype=np.bool)\n",
        "    y = np.zeros((len(sentences), len(words_list)), dtype=np.bool)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for t, word in enumerate(sentence):\n",
        "            x[i, t, word_to_index[word]] = 1\n",
        "        y[i, word_to_index[next_words[i]]] = 1\n",
        "\n",
        "    # build the model\n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(INPUT_LENGTH, len(words_list))))\n",
        "    # model.add(LSTM(len(VALID_CHARS) * 7, input_shape=(INPUT_LENGTH, len(VALID_CHARS))))\n",
        "    \n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('selu'))\n",
        "\n",
        "    # model.add(Dense(len(VALID_CHARS)*4))\n",
        "    # model.add(Activation('selu'))\n",
        "\n",
        "    # model.add(Dense(len(VALID_CHARS)*4))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Activation('selu'))\n",
        "\n",
        "    # model.add(Bidirectional(LSTM(128), input_shape=(INPUT_LENGTH, len(VALID_CHARS))))\n",
        "    model.add(Dense(len(words_list), activation='softmax'))\n",
        "\n",
        "    # optimizer = RMSprop(lr=0.01)\n",
        "    optimizer = Adam()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_crossentropy', 'accuracy'])\n",
        "\n",
        "    epochs = 10\n",
        "    \n",
        "    print_callback = LambdaCallback(on_epoch_end=lambda a, b: on_epoch_end(a, b, data, model))\n",
        "\n",
        "    # train the model\n",
        "    model.fit(x, y,\n",
        "            epochs=epochs,\n",
        "            callbacks=[print_callback]\n",
        "            )\n",
        "\n",
        "    # save and download the model\n",
        "    model.save('/content/model')\n",
        "    !zip -r /content/model.zip /content/model\n",
        "    files.download('/content/model.zip')\n",
        "\n",
        "def main():\n",
        "    # TRAIN TRUMP\n",
        "    # trump_data = get_tweets_list(TRUMP_DATA_FILE_NAME, 1000)  # TODO remove upto\n",
        "    print(\"number of trump tweets:\", len(trump_tweets))\n",
        "    train_from_data(trump_tweets)\n",
        "    # TRAIN NEWS ORGS\n",
        "    # TODO\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}