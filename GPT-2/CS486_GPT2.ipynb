{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS486 GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPe7822MFQKGCtiEnXTCmv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Da-Pen/CS486-twitter-bot/blob/main/GPT-2/CS486_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxaz_TYJEr4R"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZLra58y9_zV"
      },
      "source": [
        "# ml_model_name = os.environ.get(\"model_name\") # Usually 124M for mix between accuracy and speed\n",
        "# ml_training_steps = int(os.environ.get(\"training_steps\")) # Usually 100\n",
        "# ml_generation_length = int(os.environ.get(\"generation_length\")) # Usually 1000\n",
        "# ml_temp = float(os.environ.get(\"generation_temperature\")) # How 'crazy' the output is. Recommend between 0.7-1 for useful results or something like 2 if you wanna have fun.\n",
        "# ml_batch_size = int(os.environ.get(\"generation_batch_size\"))\n",
        "# ml_samples = int(os.environ.get(\"generation_samples_x_batch_size\")) * ml_batch_size\n",
        "\n",
        "# Setup variables from config file\n",
        "ml_model_name = \"124M\"\n",
        "ml_training_steps = 100\n",
        "ml_generation_length = 1000\n",
        "ml_temp = 1.0\n",
        "# ml_batch_size = int(os.environ.get(\"generation_batch_size\"))\n",
        "# ml_samples = int(os.environ.get(\"generation_samples_x_batch_size\")) * ml_batch_size"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGFaBpK5EoTl",
        "outputId": "d89e6a2e-deba-415d-895b-668b17fd5619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=ml_model_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 335Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 89.7Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 330Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 211Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 234Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 127Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 136Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0qg5hiqEoYJ"
      },
      "source": [
        "file_name = training_file_name #filename for training data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj1cc1gMGpn1"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,\n",
        "              file_name,\n",
        "              ml_model_name,\n",
        "              ml_training_steps\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_ldRiqmZIF"
      },
      "source": [
        "#https://towardsdatascience.com/how-to-make-a-gpt2-twitter-bot-8669df60e60a\n",
        "\n",
        "model_name = \"117M\"\n",
        "\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "    print(f\"Downloading {model_name} model...\")\n",
        "    gpt2.download_gpt2(model_name=model_name)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "num_steps = 100 #TODO: set this to Ceiling(Word Count/Line Count) of the training data file\n",
        "\n",
        "text_path = \"TRAINING DATA\"\n",
        "\n",
        "gpt2.finetune(sess, \n",
        "              text_path,\n",
        "              model_name=model_name,\n",
        "              steps=num_steps\n",
        "             )\n",
        "\n",
        "# session: The session is just the current Tensorflow session\n",
        "# dataset: This is the path to a text file to load in and use for training, more on this later.\n",
        "# model_name: The name of the GPT2 model to use can be 117M, 124M, or 355M. 124M works well on my 1080Ti GPU.\n",
        "# steps\" The number of steps for the model to take. This number should be high enough to step through your whole data set at least once but not so high that you over fit. When fine tuning GPT2 on a relatively small data set I like to do one to two epochs. Then I test to make sure that what it generates wasn’t directly from the training set.\n",
        "\n",
        "\n",
        "length = 10 \n",
        "destination_path = \"\"\n",
        "temperature = 0.5\n",
        "prefix = \"\"\n",
        "\n",
        "text = gpt2.generate(\n",
        "    sess,\n",
        "    length=length,\n",
        "    temperature=temperature,\n",
        "    destination_path=destination_path,\n",
        "    prefix=prefix,\n",
        "    return_as_list=True\n",
        ")\n",
        "\n",
        "print(text)\n",
        "# sess: is the tensorflow session we want to use\n",
        "# checkpoint_dir: is the path to the saved checkpoints from our finetuning\n",
        "# temperature: is and value greater than 0 I like to play around between .8 and 2. The lower the temperature the more consistent and predictable your outputs the higher the temperature the more wild, fun, and possibly nonsensical they will be.\n",
        "# destination_path: is a path to where you want the text to be saved. If you just want to return it inline make this None\n",
        "# prefix: is a fun one. It can be a string of text that is used to seed the model. So if you started with “thou shall not” then the model will write the next words as if it had started with “thou shall not.”\n",
        "# return_as_list: will cause the function to return the text instead of just printing it out."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}